# Translation-Latin-to-hindi-
For this , we have an input embedding size m=256, hidden state size k= 256 for both the encoder and decoder, input and output sequence length ğ‘‡=20, and vocabulary v=60. We use a single-layer LSTM for both the encoder and decoder, with greedy decoding during inference and no attention mechanism. The total number of computations performed by the network can be calculated using the formula ğ‘‡Ã—[8ğ‘˜(ğ‘š+ğ‘˜)+ğ‘˜ğ‘‰].Substituting the values, we get 20Ã—[8Ã—256Ã—(256+256)+256Ã—60]=21,278,72020.Therefore, the total number of computations done by the network is approximately 21.28 million operations.
The total number of parameters in the network is calculated using the formula 2Vm+8k(m+k+1)+kV+V.Plugging in the values, we get 2Ã—60Ã—256+8Ã—256Ã—(256+256+1)+256Ã—60+60=1,100,4642Ã—60Ã—256+8Ã—256Ã—(256+256+1)+256Ã—60+60=1,100,464 .Hence, the total number of parameters in the model is approximately 1.10 million.
